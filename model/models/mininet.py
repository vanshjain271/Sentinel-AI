# -*- coding: utf-8 -*-
"""New_Mininet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12cPMKbUDDiE6tJS2qi_k5PRvn-f-nRig
"""

!pip install -q numpy pandas scikit-learn matplotlib seaborn xgboost lightgbm
!pip install -q tensorflow torch torch-geometric shap lime alibi-detect
!pip install -q imbalanced-learn scipy statsmodels
!apt-get install -qq tshark wireshark

import numpy as np
# Compatibility patch for numpy.float deprecation in alibi-detect
# See: https://github.com/SeldonIO/alibi-detect/issues/1090
# And: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
np.float = float
import os
import pandas as pd
import random
import time
import joblib
import json
import pickle
import warnings
warnings.filterwarnings('ignore')

import matplotlib.pyplot as plt
import seaborn as sns
sns.set(style="whitegrid", palette="muted")

from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                           f1_score, roc_auc_score, confusion_matrix,
                           classification_report, precision_recall_curve)
from sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier,
                            IsolationForest, VotingClassifier)
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.calibration import CalibratedClassifierCV
import xgboost as xgb
import lightgbm as lgb

import tensorflow as tf
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import (Dense, Dropout, LSTM, GRU, Conv1D,
                                   MaxPooling1D, Flatten, Input,
                                   Bidirectional, Attention, MultiHeadAttention)
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv

import shap
import lime
import lime.lime_tabular
from alibi_detect.od import IForest, Mahalanobis

print("âœ… All packages installed and imported")
print(f"TensorFlow: {tf.__version__}")
print(f"PyTorch: {torch.__version__}")

# 2. ELITE 5G NETWORK TRAFFIC GENERATOR

class FiveGDDoSDataGenerator:
    """Generates realistic 5G network traffic with DDoS attacks"""

    def __init__(self, seed=42):
        np.random.seed(seed)
        random.seed(seed)
        self.attack_types = {
            'Normal': {'rate': 0.6, 'pps': (10, 100), 'size': (64, 1500)},
            'HTTP_Flood_5G': {'rate': 0.08, 'pps': (1000, 5000), 'size': (800, 1200)},
            'UDP_Amplification': {'rate': 0.06, 'pps': (5000, 20000), 'size': (4000, 65535)},
            'ICMP_Flood': {'rate': 0.05, 'pps': (2000, 10000), 'size': (84, 1500)},
            'Slowloris_5G': {'rate': 0.04, 'pps': (1, 10), 'size': (100, 300)},
            'TCP_SYN_Flood': {'rate': 0.07, 'pps': (5000, 15000), 'size': (60, 100)},
            'DNS_Tunneling': {'rate': 0.05, 'pps': (100, 500), 'size': (200, 800)},
            'QUIC_Flood': {'rate': 0.05, 'pps': (3000, 8000), 'size': (1200, 2000)}
        }

        # 5G Network Elements
        self.gNBs = [f"gNB_{i:03d}" for i in range(1, 51)]
        self.amfs = [f"AMF_{i:02d}" for i in range(1, 11)]
        self.upfs = [f"UPF_{i:02d}" for i in range(1, 6)]
        self.slices = ['eMBB', 'URLLC', 'mMTC', 'V2X', 'IndustrialIoT']

        # 5G QoS Parameters
        self.qci_mapping = {
            'eMBB': {'5qi': 6, 'priority': 10, 'packet_delay': 300},
            'URLLC': {'5qi': 3, 'priority': 1, 'packet_delay': 10},
            'mMTC': {'5qi': 9, 'priority': 15, 'packet_delay': 1000},
            'V2X': {'5qi': 2, 'priority': 5, 'packet_delay': 20},
            'IndustrialIoT': {'5qi': 7, 'priority': 12, 'packet_delay': 100}
        }

    def generate_ip(self, prefix="10.5"):
        """Generate realistic 5G network IPs"""
        return f"{prefix}.{random.randint(0, 255)}.{random.randint(1, 254)}"

    def generate_5g_features(self):
        """Generate 5G-specific network features"""
        slice_type = random.choice(self.slices)
        qci_info = self.qci_mapping[slice_type]

        return {
            'slice_type': slice_type,
            '5qi': qci_info['5qi'],
            'priority_level': qci_info['priority'],
            'max_delay_budget': qci_info['packet_delay'],
            'gNB_id': random.choice(self.gNBs),
            'amf_id': random.choice(self.amfs),
            'upf_id': random.choice(self.upfs),
            'nssai': f"SST{qci_info['5qi']}-SD{random.randint(100, 999):03d}",
            'tac': random.randint(1, 65535),
            'ran_ue_id': random.randint(1000000, 9999999),
            'pdu_session_id': random.randint(1, 15)
        }

    def generate_traffic_flow(self, attack_type, num_samples=1000):
        """Generate complete traffic flow for given attack type"""
        flows = []
        attack_config = self.attack_types[attack_type]

        for _ in range(num_samples):
            # 5G network context
            fiveg_context = self.generate_5g_features()

            # Source/Destination
            if attack_type == 'Normal':
                src_ip = self.generate_ip("10.5")
                dst_ip = self.generate_ip("10.6")
            else:
                src_ip = self.generate_ip("192.168")  # Attacker IP range
                dst_ip = self.generate_ip("10.5")      # 5G core IP

            # Traffic parameters based on attack type
            pps_min, pps_max = attack_config['pps']
            size_min, size_max = attack_config['size']

            packet_size = np.random.uniform(size_min, size_max)
            packets_per_second = np.random.uniform(pps_min, pps_max)

            # Time-based features
            timestamp = time.time() + random.randint(0, 86400)
            inter_arrival_time = 1.0 / packets_per_second

            # Protocol distribution
            if attack_type == 'HTTP_Flood_5G':
                protocol = 6  # TCP
                src_port = random.randint(1024, 65535)
                dst_port = random.choice([80, 443, 8080])
            elif attack_type == 'UDP_Amplification':
                protocol = 17  # UDP
                src_port = random.choice([53, 123, 161, 1900])
                dst_port = random.randint(1024, 65535)
            elif attack_type == 'QUIC_Flood':
                protocol = 17  # UDP (QUIC runs over UDP)
                src_port = random.randint(1024, 65535)
                dst_port = 443
            else:
                protocol = random.choice([6, 17, 1])
                src_port = random.randint(1024, 65535)
                dst_port = random.randint(1, 1023)

            # Advanced statistical features
            avg_packet_size = packet_size
            packet_size_variance = np.random.gamma(2, 50)
            jitter = np.random.exponential(inter_arrival_time * 0.1)

            # Connection metrics
            concurrent_connections = np.random.poisson(5 if attack_type == 'Normal' else 50)
            failed_connections = np.random.poisson(0.1 if attack_type == 'Normal' else 15)
            retransmission_rate = np.random.beta(1, 99) if attack_type == 'Normal' else np.random.beta(10, 90)

            # Entropy measures (for anomaly detection)
            src_port_entropy = -np.log2(1/65535)  # Simplified
            dst_port_entropy = -np.log2(1/1023)
            protocol_entropy = 1.0 if protocol in [6, 17] else 0.5

            flow = {
                # Basic features
                'src_ip': src_ip,
                'dst_ip': dst_ip,
                'protocol': protocol,
                'src_port': src_port,
                'dst_port': dst_port,
                'packet_size': packet_size,
                'timestamp': timestamp,
                'inter_arrival_time': inter_arrival_time,

                # Statistical features
                'avg_packet_size': avg_packet_size,
                'packet_size_variance': packet_size_variance,
                'jitter': jitter,
                'packets_per_second': packets_per_second,
                'bytes_per_second': packet_size * packets_per_second,

                # Connection features
                'concurrent_connections': concurrent_connections,
                'failed_connections': failed_connections,
                'retransmission_rate': retransmission_rate,

                # Entropy features
                'src_port_entropy': src_port_entropy,
                'dst_port_entropy': dst_port_entropy,
                'protocol_entropy': protocol_entropy,
                'packet_size_entropy': -np.log2(packet_size/65535),

                # 5G-specific features
                **fiveg_context,

                # Labels
                'attack_type': attack_type,
                'is_malicious': 0 if attack_type == 'Normal' else 1,
                'attack_category': self._categorize_attack(attack_type)
            }

            flows.append(flow)

        return flows

    def _categorize_attack(self, attack_type):
        """Categorize attacks for multi-label learning"""
        categories = {
            'Normal': 'benign',
            'HTTP_Flood_5G': 'application_layer',
            'UDP_Amplification': 'amplification',
            'ICMP_Flood': 'volumetric',
            'Slowloris_5G': 'slowloris',
            'TCP_SYN_Flood': 'protocol_exploit',
            'DNS_Tunneling': 'tunneling',
            'QUIC_Flood': 'emerging_protocol'
        }
        return categories.get(attack_type, 'unknown')

    def generate_dataset(self, samples_per_class=5000):
        """Generate complete dataset with all attack types"""
        all_data = []

        print("ğŸš€ Generating 5G DDoS Dataset...")
        for attack_type, config in self.attack_types.items():
            print(f"  â†’ {attack_type}: {samples_per_class} samples")
            flows = self.generate_traffic_flow(attack_type, samples_per_class)
            all_data.extend(flows)

        df = pd.DataFrame(all_data)

        # Calculate derived features
        df['packet_size_ratio'] = df['packet_size'] / df['avg_packet_size']
        df['connection_success_rate'] = 1 - (df['failed_connections'] / df['concurrent_connections'].replace(0, 1))
        df['traffic_intensity'] = df['packets_per_second'] * df['packet_size'] / 1000000  # Mbps

        print(f"\nâœ… Dataset generated: {df.shape[0]} samples, {df.shape[1]} features")
        print(f"ğŸ“Š Class distribution:")
        print(df['attack_type'].value_counts())

        return df

# 3. GENERATE THE DATASET

generator = FiveGDDoSDataGenerator()
df = generator.generate_dataset(samples_per_class=3000)

# Save raw dataset
df.to_csv('5g_ddos_elite_dataset.csv', index=False)
print("ğŸ’¾ Dataset saved as '5g_ddos_elite_dataset.csv'")

# 4. FEATURE ENGINEERING PIPELINE

class FeatureEngineeringPipeline:
    """Advanced feature engineering for 5G DDoS detection"""

    def __init__(self):
        self.numerical_features = None
        self.categorical_features = None

    def prepare_features(self, df):
        """Prepare all features for modeling"""

        # Separate features and labels
        X = df.drop(['attack_type', 'attack_category', 'src_ip', 'dst_ip'], axis=1, errors='ignore')
        y = df['is_malicious']

        # Encode categorical features
        categorical_cols = ['slice_type', 'gNB_id', 'amf_id', 'upf_id', 'nssai']
        for col in categorical_cols:
            if col in X.columns:
                X[col] = X[col].astype('category').cat.codes

        # Identify numerical features
        self.numerical_features = [col for col in X.columns if X[col].dtype in ['int64', 'float64']]
        self.categorical_features = [col for col in X.columns if col not in self.numerical_features]

        # Handle missing values
        X = X.fillna(0)

        # Remove highly correlated features
        corr_matrix = X[self.numerical_features].corr().abs()
        upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
        to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]
        X = X.drop(to_drop, axis=1)

        print(f"ğŸ“ Final feature shape: {X.shape}")
        print(f"ğŸ¯ Target distribution: {y.value_counts().to_dict()}")

        return X, y, X.columns.tolist()

# Apply feature engineering
pipeline = FeatureEngineeringPipeline()
X, y, feature_names = pipeline.prepare_features(df)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Save scaler and feature names
joblib.dump(scaler, '5g_ddos_scaler.pkl')
joblib.dump(feature_names, '5g_feature_names.pkl')

print("âœ… Features prepared and scaled")

import xgboost as xgb
from xgboost.callback import EarlyStopping as XGBEarlyStopping # Alias XGBoost's EarlyStopping

class EliteModelFactory:
    """Trains and saves all elite models"""

    def __init__(self, X_train, X_test, y_train, y_test, feature_names):
        self.X_train = X_train
        self.X_test = X_test
        self.y_train = y_train
        self.y_test = y_test
        self.feature_names = feature_names
        self.models = {}
        self.results = {}

    def train_random_forest_advanced(self):
        """Enhanced Random Forest with feature importance analysis"""
        print("ğŸŒ² Training Advanced Random Forest...")
        rf = RandomForestClassifier(
            n_estimators=200,
            max_depth=20,
            min_samples_split=5,
            min_samples_leaf=2,
            max_features='sqrt',
            bootstrap=True,
            random_state=42,
            n_jobs=-1,
            class_weight='balanced'
        )

        # Train
        rf.fit(self.X_train, self.y_train)

        # Evaluate
        y_pred = rf.predict(self.X_test)
        y_prob = rf.predict_proba(self.X_test)[:, 1]

        self.results['RandomForest'] = {
            'accuracy': accuracy_score(self.y_test, y_pred),
            'precision': precision_score(self.y_test, y_pred),
            'recall': recall_score(self.y_test, y_pred),
            'f1': f1_score(self.y_test, y_pred),
            'roc_auc': roc_auc_score(self.y_test, y_prob),
            'model': rf
        }

        # Save feature importance
        importances = pd.DataFrame({
            'feature': self.feature_names,
            'importance': rf.feature_importances_
        }).sort_values('importance', ascending=False)

        importances.to_csv('rf_feature_importance.csv', index=False)

        # Save model
        joblib.dump(rf, 'models/random_forest_elite.pkl')
        print(f"  â†’ Accuracy: {self.results['RandomForest']['accuracy']:.4f}")

        return rf

    def train_xgboost_ensemble(self):
        """XGBoost with careful hyperparameter tuning"""
        print("ğŸš€ Training XGBoost Ensemble...")

        xgb_model = xgb.XGBClassifier(
            n_estimators=150,
            max_depth=8,
            learning_rate=0.1,
            subsample=0.8,
            colsample_bytree=0.8,
            reg_alpha=0.1,
            reg_lambda=1.0,
            random_state=42,
            use_label_encoder=False,
            eval_metric='logloss',
            scale_pos_weight=len(self.y_train[self.y_train==0])/len(self.y_train[self.y_train==1])
        )

        # Train without early stopping to avoid TypeError
        xgb_model.fit(
            self.X_train, self.y_train,
            eval_set=[(self.X_test, self.y_test)],
            verbose=False # Keep verbose to suppress output during training
        )

        # Evaluate
        y_pred = xgb_model.predict(self.X_test)
        y_prob = xgb_model.predict_proba(self.X_test)[:, 1]

        self.results['XGBoost'] = {
            'accuracy': accuracy_score(self.y_test, y_pred),
            'precision': precision_score(self.y_test, y_pred),
            'recall': recall_score(self.y_test, y_pred),
            'f1': f1_score(self.y_test, y_pred),
            'roc_auc': roc_auc_score(self.y_test, y_prob),
            'model': xgb_model
        }

        # Save model
        xgb_model.save_model('models/xgboost_elite.json')
        joblib.dump(xgb_model, 'models/xgboost_elite.pkl')
        print(f"  â†’ Accuracy: {self.results['XGBoost']['accuracy']:.4f}")

        return xgb_model

    def train_lstm_temporal(self):
        """LSTM for temporal pattern recognition"""
        print("â³ Training LSTM Temporal Model...")

        timesteps = 10
        num_samples_train = self.X_train.shape[0]
        num_samples_test = self.X_test.shape[0]
        num_features = self.X_train.shape[1]

        # Prepare training data
        # Ensure number of samples is divisible by timesteps
        remainder_train = num_samples_train % timesteps
        X_train_chopped = self.X_train if remainder_train == 0 else self.X_train[:-remainder_train]
        y_train_chopped = self.y_train if remainder_train == 0 else self.y_train[:-remainder_train]

        # Reshape for LSTM: (num_sequences, timesteps, num_features)
        # Each sequence of 'timesteps' data points will have 'num_features'
        X_train_reshaped = X_train_chopped.reshape(-1, timesteps, num_features)

        # For simplicity, taking the label of the last sample in each sequence.
        # This assumes the label for the sequence is determined by its final state.
        y_train_reshaped = y_train_chopped.iloc[timesteps-1::timesteps].values

        # Prepare test data similarly
        remainder_test = num_samples_test % timesteps
        X_test_chopped = self.X_test if remainder_test == 0 else self.X_test[:-remainder_test]
        y_test_chopped = self.y_test if remainder_test == 0 else self.y_test[:-remainder_test]

        X_test_reshaped = X_test_chopped.reshape(-1, timesteps, num_features)
        y_test_reshaped = y_test_chopped.iloc[timesteps-1::timesteps].values

        # Check if reshaped data is empty before training
        if X_train_reshaped.shape[0] == 0 or y_train_reshaped.shape[0] == 0:
            print("  â†’ Skipping LSTM training: Not enough data for reshaping with current timesteps.")
            self.results['LSTM'] = {
                'accuracy': 0, 'precision': 0, 'recall': 0, 'f1': 0, 'roc_auc': 0,
                'model': None, 'history': None
            }
            return None

        # Build LSTM model
        lstm_model = Sequential([
            LSTM(128, input_shape=(timesteps, num_features),
                 return_sequences=True, dropout=0.3),
            LSTM(64, return_sequences=False, dropout=0.3),
            Dense(32, activation='relu'),
            Dropout(0.2),
            Dense(1, activation='sigmoid')
        ])

        lstm_model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='binary_crossentropy',
            metrics=['accuracy', tf.keras.metrics.AUC()]
        )

        # Train
        history = lstm_model.fit(
            X_train_reshaped, y_train_reshaped,
            epochs=20,
            batch_size=64,
            validation_split=0.2,
            verbose=0,
            callbacks=[
                tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),
                tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=3)
            ]
        )

        # Evaluate
        y_prob = lstm_model.predict(X_test_reshaped).flatten()
        y_pred = (y_prob > 0.5).astype(int)

        self.results['LSTM'] = {
            'accuracy': accuracy_score(y_test_reshaped, y_pred),
            'precision': precision_score(y_test_reshaped, y_pred),
            'recall': recall_score(y_test_reshaped, y_pred),
            'f1': f1_score(y_test_reshaped, y_pred),
            'roc_auc': roc_auc_score(y_test_reshaped, y_prob),
            'model': lstm_model,
            'history': history.history
        }

        # Save model
        lstm_model.save('models/lstm_elite.keras')
        print(f"  â†’ Accuracy: {self.results['LSTM']['accuracy']:.4f}")

        return lstm_model

    def train_ensemble_voting(self):
        """Voting ensemble of top models"""
        print("ğŸ¤ Training Voting Ensemble...")

        # Get trained models
        rf = self.results['RandomForest']['model']
        xgb_model = self.results['XGBoost']['model']

        # Create voting classifier
        voting_clf = VotingClassifier(
            estimators=[
                ('rf', rf),
                ('xgb', xgb_model)
            ],
            voting='soft',
            weights=[1.5, 2.0]  # Weight XGBoost higher
        )

        voting_clf.fit(self.X_train, self.y_train)

        # Evaluate
        y_pred = voting_clf.predict(self.X_test)
        y_prob = voting_clf.predict_proba(self.X_test)[:, 1]

        self.results['Ensemble'] = {
            'accuracy': accuracy_score(self.y_test, y_pred),
            'precision': precision_score(self.y_test, y_pred),
            'recall': recall_score(self.y_test, y_pred),
            'f1': f1_score(self.y_test, y_pred),
            'roc_auc': roc_auc_score(self.y_test, y_prob),
            'model': voting_clf
        }

        # Save model
        joblib.dump(voting_clf, 'models/ensemble_voting_elite.pkl')
        print(f"  â†’ Accuracy: {self.results['Ensemble']['accuracy']:.4f}")

        return voting_clf

    def train_autoencoder_anomaly(self):
        """Autoencoder for unsupervised anomaly detection"""
        print("ğŸ” Training Autoencoder for Anomaly Detection...")

        # Normal data only for training autoencoder
        normal_indices = np.where(self.y_train == 0)[0]
        X_normal = self.X_train[normal_indices]

        # Build autoencoder
        input_dim = X_normal.shape[1]
        encoding_dim = 32

        input_layer = Input(shape=(input_dim,))
        encoder = Dense(encoding_dim * 2, activation="relu")(input_layer)
        encoder = Dense(encoding_dim, activation="relu")(encoder)

        decoder = Dense(encoding_dim * 2, activation="relu")(encoder)
        decoder = Dense(input_dim, activation="linear")(decoder)

        autoencoder = Model(inputs=input_layer, outputs=decoder)
        autoencoder.compile(optimizer='adam', loss='mse')

        # Train
        autoencoder.fit(
            X_normal, X_normal,
            epochs=50,
            batch_size=256,
            validation_split=0.1,
            verbose=0,
            shuffle=True
        )

        # Calculate reconstruction error
        reconstructions = autoencoder.predict(self.X_test)
        mse = np.mean(np.power(self.X_test - reconstructions, 2), axis=1)

        # Use reconstruction error as anomaly score
        threshold = np.percentile(mse[self.y_test == 0], 95)
        y_pred_ae = (mse > threshold).astype(int)

        self.results['Autoencoder'] = {
            'accuracy': accuracy_score(self.y_test, y_pred_ae),
            'precision': precision_score(self.y_test, y_pred_ae),
            'recall': recall_score(self.y_test, y_pred_ae),
            'f1': f1_score(self.y_test, y_pred_ae),
            'roc_auc': roc_auc_score(self.y_test, y_pred_ae),
            'model': autoencoder,
            'threshold': threshold
        }

        # Save model
        autoencoder.save('models/autoencoder_elite.keras')
        print(f"  â†’ Accuracy: {self.results['Autoencoder']['accuracy']:.4f}")

        return autoencoder

    def train_all_models(self):
        """Train all elite models"""
        print("\n" + "="*60)
        print("ğŸ­ ELITE MODEL FACTORY - TRAINING ALL MODELS")
        print("="*60)

        # Create models directory
        !mkdir -p models

        # Train each model
        self.train_random_forest_advanced()
        self.train_xgboost_ensemble()
        self.train_lstm_temporal()
        self.train_autoencoder_anomaly()
        self.train_ensemble_voting()

        # Save results summary
        results_df = pd.DataFrame(self.results).T
        results_df.to_csv('model_performance_summary.csv')

        print("\nâœ… All elite models trained and saved!")
        return results_df

# 6. TRAIN ALL MODELS

# Create model factory
model_factory = EliteModelFactory(X_train_scaled, X_test_scaled, y_train, y_test, feature_names)

# Train all models
results = model_factory.train_all_models()

# 7. SHAP EXPLAINABILITY

def generate_shap_explanations(model, X_sample_scaled, feature_names, X_train_scaled):
    """Generate SHAP explanations for model interpretability using shap.Explainer"""
    print("\nğŸ§  Generating SHAP Explanations (Using shap.Explainer)...")

    # Sample data for SHAP (faster computation)
    X_sample_shap_df = pd.DataFrame(X_sample_scaled[:100], columns=feature_names)
    X_train_df = pd.DataFrame(X_train_scaled, columns=feature_names)

    # Use shap.Explainer (newer API) with background data
    explainer = shap.Explainer(model, X_train_df)
    shap_values = explainer(X_sample_shap_df)

    # For binary classification, shap_values.values will be (num_samples, num_features, num_classes)
    # We want the positive class (index 1) SHAP values
    shap_values_for_plot = shap_values.values[:, :, 1] # SHAP values for positive class
    base_value_for_plot = shap_values.base_values[:, 1] # Base value for positive class

    # Debug prints
    print(f"DEBUG: shap_values.values.shape: {shap_values.values.shape}")
    print(f"DEBUG: shap_values_for_plot.shape: {shap_values_for_plot.shape}")
    print(f"DEBUG: X_sample_shap_df.shape: {X_sample_shap_df.shape}")

    # Summary plot
    plt.figure(figsize=(12, 8))
    shap.summary_plot(shap_values_for_plot, X_sample_shap_df, show=False)
    plt.title('SHAP Feature Importance (Positive Class)', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.savefig('shap_summary.png', dpi=150, bbox_inches='tight')

    # Force plot for a single prediction of the positive class
    plt.figure(figsize=(10, 4))

    # Ensure base_value_for_plot is a scalar for single force plot
    single_base_value = base_value_for_plot[0]

    shap.force_plot(
        single_base_value,
        shap_values_for_plot[0],
        X_sample_shap_df.iloc[0], # Pass a pandas Series for single instance data
        show=False,
        matplotlib=True
    )
    plt.title('SHAP Force Plot - Single Prediction (Positive Class)', fontsize=14)
    plt.tight_layout()
    plt.savefig('shap_force_plot.png', dpi=150, bbox_inches='tight')

    # Save SHAP values for the positive class
    shap_df_output = pd.DataFrame(shap_values_for_plot, columns=feature_names)
    shap_df_output.to_csv('shap_values.csv', index=False)

    print("âœ… SHAP explanations saved as images and CSV")
    return explainer

# Generate SHAP for Random Forest
rf_model = results.loc['RandomForest', 'model']
explainer = generate_shap_explanations(rf_model, X_test_scaled, feature_names, X_train_scaled)

# 8. CREATE MODEL METADATA AND CONFIG

# Create model metadata
model_metadata = {
    'created_at': time.strftime('%Y-%m-%d %H:%M:%S'),
    'dataset_info': {
        'total_samples': len(df),
        'features_count': len(feature_names),
        'malicious_ratio': y.mean(),
        'attack_types': df['attack_type'].nunique() if 'attack_type' in df.columns else 1
    },
    'models_trained': list(results.index),
    'performance_summary': results[['accuracy', 'f1', 'roc_auc']].to_dict(),
    'feature_importance': {
        'top_5_features': pd.read_csv('rf_feature_importance.csv').head(5).to_dict('records')
    },
    'usage_instructions': {
        'scaler': '5g_ddos_scaler.pkl',
        'feature_names': '5g_feature_names.pkl',
        'ensemble_model': 'models/ensemble_voting_elite.pkl',
        'anomaly_detector': 'models/autoencoder_elite.keras'
    }
}

# Save metadata
with open('model_metadata.json', 'w') as f:
    json.dump(model_metadata, f, indent=2)

print("\nğŸ“‹ Model Metadata Saved:")
print(json.dumps(model_metadata, indent=2))

# 9. FINAL VALIDATION AND TESTING

print("\n" + "="*60)
print("ğŸ¯ FINAL MODEL VALIDATION")
print("="*60)

# Load ensemble model for final test
ensemble_model = joblib.load('models/ensemble_voting_elite.pkl')

# Make predictions on test set
y_pred_final = ensemble_model.predict(X_test_scaled)
y_prob_final = ensemble_model.predict_proba(X_test_scaled)[:, 1]

# Comprehensive evaluation
print("\nğŸ“Š Ensemble Model Performance:")
print(f"Accuracy:  {accuracy_score(y_test, y_pred_final):.4f}")
print(f"Precision: {precision_score(y_test, y_pred_final):.4f}")
print(f"Recall:    {recall_score(y_test, y_pred_final):.4f}")
print(f"F1-Score:  {f1_score(y_test, y_pred_final):.4f}")
print(f"ROC-AUC:   {roc_auc_score(y_test, y_prob_final):.4f}")

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred_final)
print(f"\nğŸ“ˆ Confusion Matrix:")
print(cm)

# Classification Report
print("\nğŸ“‹ Classification Report:")
print(classification_report(y_test, y_pred_final, target_names=['Normal', 'DDoS']))

# 10. EXPORT FOR PRODUCTION

print("\n" + "="*60)
print("ğŸ“¦ EXPORTING FOR PRODUCTION USE")
print("="*60)

# Create production package
!mkdir -p production_package
!cp models/*.pkl production_package/
!cp models/*.json production_package/
!cp models/*.keras production_package/
!cp *.pkl production_package/
!cp model_metadata.json production_package/

# Create requirements file
requirements = """
numpy>=1.21.0
pandas>=1.3.0
scikit-learn>=1.0.0
xgboost>=1.5.0
lightgbm>=3.3.0
tensorflow>=2.8.0
shap>=0.40.0
joblib>=1.1.0
"""

with open('production_package/requirements.txt', 'w') as f:
    f.write(requirements)

# Create inference script
inference_script = '''
import joblib
import numpy as np
import pandas as pd

class EliteDDoSDetector:
    """Production-ready DDoS detector using elite models"""

    def __init__(self, model_path='ensemble_voting_elite.pkl'):
        self.scaler = joblib.load('5g_ddos_scaler.pkl')
        self.feature_names = joblib.load('5g_feature_names.pkl')
        self.model = joblib.load(model_path)

    def predict(self, features):
        """Predict if traffic is DDoS"""
        # Scale features
        features_scaled = self.scaler.transform([features])

        # Get prediction and probability
        prediction = self.model.predict(features_scaled)[0]
        probability = self.model.predict_proba(features_scaled)[0][1]

        return {
            'is_ddos': bool(prediction),
            'confidence': float(probability),
            'risk_level': 'HIGH' if probability > 0.8 else 'MEDIUM' if probability > 0.6 else 'LOW'
        }

# Usage example:
# detector = EliteDDoSDetector()
# result = detector.predict(feature_array)
'''

with open('production_package/inference.py', 'w') as f:
    f.write(inference_script)

print("âœ… Production package created in 'production_package/' folder")
print("\nğŸ“ Files generated:")
print("  - 5g_ddos_elite_dataset.csv (Full dataset)")
print("  - models/random_forest_elite.pkl")
print("  - models/xgboost_elite.pkl")
print("  - models/lstm_elite.keras")
print("  - models/autoencoder_elite.keras")
print("  - models/ensemble_voting_elite.pkl")
print("  - 5g_ddos_scaler.pkl")
print("  - 5g_feature_names.pkl")
print("  - shap_summary.png (Feature importance)")
print("  - model_metadata.json")
print("  - production_package/ (Ready for deployment)")

print("\nğŸ‰ ELITE MODEL FACTORY COMPLETE!")
print("All models are trained, validated, and ready for your SentinelAI system.")